{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import library\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import librosa\n",
    "import os\n",
    "from pydub import AudioSegment\n",
    "from PIL import Image\n",
    "from scipy.stats import pearsonr, kendalltau\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to DataCleaner Pro!\n",
      "Your trusted partner for automated data cleaning services.\n",
      "We ensure your data is pristine, accurate, and ready for analysis.\n",
      "Let us handle the cleaning, so you can focus on insights.\n",
      "\n",
      "\n",
      "Choose the type of dataset you want to clean:\n",
      "1. Tabular Data\n",
      "2. Image Samples (Placeholder)\n",
      "3. Audio Samples\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'tabular_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/_k/4l462p5917dcr7f48lj53rmc0000gn/T/ipykernel_35957/2446047734.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;31m# Load data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0mloaded_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m \u001b[0mcleaned_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreduce_missing_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtabular_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0mcleaned_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;31m# Rest of your data cleaning script goes here...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tabular_data' is not defined"
     ]
    }
   ],
   "source": [
    "# Function to load tabular data\n",
    "def load_tabular_data():\n",
    "    data_path = input(\"Please enter the path to your tabular data file (CSV, Excel, etc.): \")\n",
    "    try:\n",
    "        tabular_data = pd.read_csv(data_path)  # Change this line based on the file format\n",
    "        return tabular_data \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading tabular data: {e}\")\n",
    "        return None\n",
    "def reduce_missing_values(tabular_data, threshold = 0.5):\n",
    "    return tabular_data.dropna(thresh=len(tabular_data) * threshold, axis = 1)\n",
    "\n",
    "\n",
    "# Function to load image data\n",
    "def load_image_data():\n",
    "    folder_path = input(\"Please enter the path to your image folder: \")\n",
    "    \n",
    "    try:\n",
    "        # Get a list of all files in the folder\n",
    "        image_files = [f for f in os.listdir(folder_path) if f.lower().endswith(('.jpg', '.jpeg', '.png', '.gif'))]\n",
    "\n",
    "        # Iterate through each image file and display it\n",
    "        for image_file in image_files:\n",
    "            image_path = os.path.join(folder_path, image_file)\n",
    "            image = Image.open(image_path)\n",
    "            return image.show()\n",
    "    except Exception as e:\n",
    "        print(f\"Error opening images: {e}\")\n",
    "    \n",
    "\n",
    "# Function to load audio samples\n",
    "def load_audio_data():\n",
    "    audio_path = input(\"Please enter the path to your audio file: \")\n",
    "    try:\n",
    "        audio = AudioSegment.from_file(audio_path)\n",
    "        return audio.raw_data, audio.frame_rate\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading audio data: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to handle dataset loading based on user input\n",
    "def load_data():\n",
    "    print(\"Choose the type of dataset you want to clean:\")\n",
    "    print(\"1. Tabular Data\")\n",
    "    print(\"2. Image Samples (Placeholder)\")\n",
    "    print(\"3. Audio Samples\")\n",
    "\n",
    "    choice = input(\"Enter the number corresponding to your dataset type: \")\n",
    "\n",
    "    if choice == \"1\":\n",
    "        return load_tabular_data()\n",
    "    elif choice == \"2\":\n",
    "        return load_image_data()\n",
    "    elif choice == \"3\":\n",
    "        return load_audio_data()\n",
    "    else:\n",
    "        print(\"Invalid choice. Please choose a valid dataset type (1, 2, or 3).\")\n",
    "        return None\n",
    "\n",
    "# Automated Data Cleaning Script\n",
    "#Tabular Dataset \n",
    "    #Reduce the number with large amount of missing value \n",
    "    #Reduce the number of columns by using dropping highly correlated column \n",
    "    #Interpolation using polynomial technique\n",
    "    #Using Linear Regression \n",
    "    # To find which column need to be splitted\n",
    "    # Converting to non string value to float\n",
    "    #PCA\n",
    "#Image Sample\n",
    "    #Average blur\n",
    "    #Gaussian blur\n",
    "    #Median blur\n",
    "    #Bilateral blur\n",
    "    #Salt Pepper noise removable \n",
    "    #Missing Region generation \n",
    "    #Read in image and convert to grayscale\n",
    "    #Meshgrid of pixel coordinate \n",
    "    # Create a figure of nrows x ncols subplots, and orient it appropriately\n",
    "    # for the aspect ratio of the image.\n",
    "    # Convert an integer i to coordinates in the ax array\n",
    "    # Sample 100, 1,000, 10,000 and 100,000 points and plot the interpolated\n",
    "    # images in the figure\n",
    "    #Night vision (enhancing dark images)\n",
    "#Audio Sample\n",
    "\n",
    "\n",
    "# Welcome message\n",
    "def greet_customer():\n",
    "    print(\"Welcome to DataCleaner Pro!\")\n",
    "    print(\"Your trusted partner for automated data cleaning services.\")\n",
    "    print(\"We ensure your data is pristine, accurate, and ready for analysis.\")\n",
    "    print(\"Let us handle the cleaning, so you can focus on insights.\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "# Call the greeting function\n",
    "greet_customer()\n",
    "\n",
    "# Load data\n",
    "loaded_data = load_data()\n",
    "cleaned_data = reduce_missing_values(tabular_data)\n",
    "cleaned_data\n",
    "# Rest of your data cleaning script goes here...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import kendalltau\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import VarianceThreshold, RFE\n",
    "from sklearn.impute import KNNImputer\n",
    "from scipy.stats import kendalltau\n",
    "#import pandas_profiling\n",
    "import sweetviz\n",
    "import great_expectations as ge\n",
    "from fancyimpute import KNN\n",
    "from PIL import Image, ImageFilter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pearson Correlation Matrix:\n",
      "          Age  Salary\n",
      "Age     1.000   0.975\n",
      "Salary  0.975   1.000\n",
      "\n",
      "Kendall Tau Correlation Matrix:\n",
      "        Age Salary\n",
      "Age     1.0    1.0\n",
      "Salary  1.0    1.0\n"
     ]
    }
   ],
   "source": [
    "# Algorithm 1: Checking correlation between columns (Pearson and Kendall Tau)\n",
    "def check_correlation(data):\n",
    "    pearson_correlation = data.corr(method='pearson')\n",
    "    # Selecting numeric columns for Kendall Tau\n",
    "    numeric_columns = data.select_dtypes(include=np.number).columns\n",
    "    kendall_tau_correlation = pd.DataFrame(index=numeric_columns, columns=numeric_columns)\n",
    "    \n",
    "    for col1 in numeric_columns:\n",
    "        for col2 in numeric_columns:\n",
    "            kendall_tau_correlation.loc[col1, col2], _ = kendalltau(data[col1], data[col2])\n",
    "    \n",
    "    return pearson_correlation, kendall_tau_correlation\n",
    "\n",
    "data = pd.read_csv(\"tabular_data.csv\")\n",
    "# Test the algorithms\n",
    "pearson_correlation, kendall_tau_correlation = check_correlation(data)\n",
    "print(\"\\nPearson Correlation Matrix:\")\n",
    "print(pearson_correlation)\n",
    "print(\"\\nKendall Tau Correlation Matrix:\")\n",
    "print(kendall_tau_correlation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No column with missing value\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def reduce_missing_values(data, threshold=0.5):\n",
    "    return data.dropna(thresh=len(data) * threshold, axis=1)\n",
    "\n",
    "def drop_highly_correlated_columns(data, correlation_threshold=0.9):\n",
    "    corr_matrix = data.corr().abs()\n",
    "    upper_triangle = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "    to_drop = [column for column in upper_triangle.columns if any(upper_triangle[column] > correlation_threshold)]\n",
    "    return data.drop(to_drop, axis=1)\n",
    "def interpolate_missing_values(data, method='polynomial'):\n",
    "    if method == 'polynomial':\n",
    "        return data.interpolate(method='polynomial', order=2)\n",
    "    else:\n",
    "        return data.interpolate()\n",
    "\n",
    "def impute_with_linear_regression(data):\n",
    "    # Find the first column with missing values as the target column\n",
    "    column_with_missing_value = data.columns[data.isnull().any()]\n",
    "    if column_with_missing_value.empty:\n",
    "        print(\"No column with missing value\")\n",
    "        return data\n",
    "    if not target_column:\n",
    "        target_column = column_with_missing_value[0]\n",
    "\n",
    "    target = data[target_column]\n",
    "    features = data.drop(columns=[target_column])\n",
    "\n",
    "    missing_values = data[data[target_column].isnull()]\n",
    "    if missing_values.empty:\n",
    "        return data\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)\n",
    "\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    predicted_values = model.predict(missing_values.drop(columns=[target_column]))\n",
    "\n",
    "    data_copy = data.copy()\n",
    "    data_copy.loc[data_copy[target_column].isnull(), target_column] = predicted_values\n",
    "\n",
    "    return data_copy\n",
    "\n",
    "\n",
    "def perform_pca(data, n_components=2):\n",
    "    # Separate numeric and categorical columns\n",
    "    numeric_columns = data.select_dtypes(include=['number']).columns\n",
    "    categorical_columns = data.select_dtypes(exclude=['number']).columns\n",
    "    # Convert categorical columns to numeric using label encoding\n",
    "    label_encoder = LabelEncoder()\n",
    "    data[categorical_columns] = data[categorical_columns].apply(label_encoder.fit_transform)\n",
    "    # Apply standard scaling to numeric columns\n",
    "    scaler = StandardScaler()\n",
    "    data[numeric_columns] = scaler.fit_transform(data[numeric_columns])\n",
    "    pca = PCA(n_components=n_components)\n",
    "    principal_components = pca.fit_transform(data)\n",
    "    return pd.DataFrame(data=principal_components, columns=[f'Principal_Component_{i+1}' for i in range(n_components)])\n",
    "\n",
    "def automated_data_cleaning_tabular(input_file_path, output_file_path):\n",
    "    # Read the input file\n",
    "    tabular_data = pd.read_csv(input_file_path)\n",
    "\n",
    "    # Perform data cleaning tasks\n",
    "    tabular_data_missing_values = reduce_missing_values(tabular_data)\n",
    "    tabular_data_no_correlation = drop_highly_correlated_columns(tabular_data_missing_values)\n",
    "    tabular_data_interpolated = interpolate_missing_values(tabular_data_no_correlation)\n",
    "    tabular_data_imputed = impute_with_linear_regression(tabular_data_interpolated)\n",
    "    #tabular_data_float = convert_to_float(tabular_data_imputed, ['column1', 'column2'])  # Specify columns to convert\n",
    "    tabular_data_pca = perform_pca(tabular_data_imputed, n_components=2)\n",
    "\n",
    "    # Save the cleaned data to a new file\n",
    "    tabular_data_pca.to_csv(output_file_path, index=False)\n",
    "\n",
    "# Example usage\n",
    "input_file_path = 'tabular_data.csv'\n",
    "output_file_path = 'cleaned_data.csv'\n",
    "automated_data_cleaning_tabular(input_file_path, output_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing we will provide for our data owner \n",
    "    #1. We just working with three dataset type such:\n",
    "        #Tabular dataset\n",
    "        #Image Sample\n",
    "        #Audio Sample\n",
    "        #In the case of Tabular dataset, what will provide for you in data cleaning:\n",
    "            #1. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Choose the type of dataset you want to clean:\n",
      "1. Tabular Data\n",
      "2. Image Samples (Placeholder)\n",
      "3. Audio Samples\n",
      "Error loading tabular data: [Errno 2] No such file or directory: ''\n"
     ]
    }
   ],
   "source": [
    "# Function to handle dataset loading based on user input\n",
    "def load_data():\n",
    "    print(\"Choose the type of dataset you want to clean:\")\n",
    "    print(\"1. Tabular Data\")\n",
    "    print(\"2. Image Samples (Placeholder)\")\n",
    "    print(\"3. Audio Samples\")\n",
    "\n",
    "    choice = input(\"Enter the number corresponding to your dataset type: \")\n",
    "\n",
    "    if choice == \"1\":\n",
    "        return load_tabular_data()\n",
    "    elif choice == \"2\":\n",
    "        return load_image_data()\n",
    "    elif choice == \"3\":\n",
    "        return load_audio_data()\n",
    "    else:\n",
    "        print(\"Invalid choice. Please choose a valid dataset type (1, 2, or 3).\")\n",
    "        return None\n",
    "load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cleaned_tabular() missing 2 required positional arguments: 'initial_file' and 'cleaned_dataset'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/_k/4l462p5917dcr7f48lj53rmc0000gn/T/ipykernel_35957/2807843026.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0mtabular_drop_highly_correlated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdrop_highly_correlated_columns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduce_missing_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0mcleaned_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtabular_drop_highly_correlated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m \u001b[0mcleaned_tabular\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: cleaned_tabular() missing 2 required positional arguments: 'initial_file' and 'cleaned_dataset'"
     ]
    }
   ],
   "source": [
    "# Function to load tabular data\n",
    "def load_tabular_data():\n",
    "    data_path = input(\"Please enter the path to your tabular data file (CSV, Excel, etc.): \")\n",
    "    try:\n",
    "        tabular_data = pd.read_csv(data_path)  # Change this line based on the file format\n",
    "        return reduce_missing_values\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading tabular data: {e}\")\n",
    "        return None\n",
    "def reduce_missing_values(tabular_data, threshold = 0.5):\n",
    "    return tabular_data.dropna(thresh=len(tabular_data) * threshold, axis = 1)\n",
    "\n",
    "def drop_highly_correlated_columns(data, correlation_threshold=0.9):\n",
    "    corr_matrix = data.corr().abs()\n",
    "    upper_triangle = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "    to_drop = [column for column in upper_triangle.columns if any(upper_triangle[column] > correlation_threshold)]\n",
    "    return data.drop(to_drop, axis=1)\n",
    "def interpolate_missing_values(data, method='polynomial'):\n",
    "    if method == 'polynomial':\n",
    "        return data.interpolate(method='polynomial', order=2)\n",
    "    else:\n",
    "        return data.interpolate()\n",
    "# Function to handle dataset loading based on user input\n",
    "def load_data():\n",
    "    print(\"Choose the type of dataset you want to clean:\")\n",
    "    print(\"1. Tabular Data\")\n",
    "    print(\"2. Image Samples (Placeholder)\")\n",
    "    print(\"3. Audio Samples\")\n",
    "\n",
    "    choice = input(\"Enter the number corresponding to your dataset type: \")\n",
    "\n",
    "    if choice == \"1\":\n",
    "        return load_tabular_data()\n",
    "    elif choice == \"2\":\n",
    "        return load_image_data()\n",
    "    elif choice == \"3\":\n",
    "        return load_audio_data()\n",
    "    else:\n",
    "        print(\"Invalid choice. Please choose a valid dataset type (1, 2, or 3).\")\n",
    "        return None\n",
    "def cleaned_tabular(initial_file, cleaned_dataset):\n",
    "    initial_file = load_data()\n",
    "    tabular_reduce_missing_value = reduce_missing_values(initial_file)\n",
    "    tabular_drop_highly_correlated = drop_highly_correlated_columns(reduce_missing_values)\n",
    "    cleaned_data = tabular_drop_highly_correlated()\n",
    "cleaned_tabular()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageFilter\n",
    "from PIL import Image, ImageOps\n",
    "from PIL import Image, ImageFilter\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Choose the type of dataset you want to clean:\n",
      "1. Tabular Data\n",
      "2. Image Samples (Placeholder)\n",
      "3. Audio Samples\n",
      "Error applying Average blur: expected str, bytes or os.PathLike object, not NoneType\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Function to load tabular data\n",
    "def load_tabular_data():\n",
    "    data_path = input(\"Please enter the path to your tabular data file (CSV, Excel, etc.): \")\n",
    "    try:\n",
    "        tabular_data = pd.read_csv(data_path)  # Change this line based on the file format\n",
    "        return tabular_data\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading tabular data: {e}\")\n",
    "        return None\n",
    "\n",
    "def reduce_missing_values(tabular_data, threshold=0.5):\n",
    "    return tabular_data.dropna(thresh=len(tabular_data) * threshold, axis=1)\n",
    "\n",
    "def drop_highly_correlated_columns(data, correlation_threshold=0.9):\n",
    "    corr_matrix = data.corr().abs()\n",
    "    upper_triangle = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "    to_drop = [column for column in upper_triangle.columns if any(upper_triangle[column] > correlation_threshold)]\n",
    "    return data.drop(to_drop, axis=1)\n",
    "\n",
    "def interpolate_missing_values(data, method='polynomial'):\n",
    "    if method == 'polynomial':\n",
    "        return data.interpolate(method='polynomial', order=2)\n",
    "    else:\n",
    "        return data.interpolate()\n",
    "\n",
    "\n",
    "def perform_pca(data, n_components=2):\n",
    "    # Separate numeric and categorical columns\n",
    "    numeric_columns = data.select_dtypes(include=['number']).columns\n",
    "    categorical_columns = data.select_dtypes(exclude=['number']).columns\n",
    "    # Convert categorical columns to numeric using label encoding\n",
    "    label_encoder = LabelEncoder()\n",
    "    data[categorical_columns] = data[categorical_columns].apply(label_encoder.fit_transform)\n",
    "    # Apply standard scaling to numeric columns\n",
    "    scaler = StandardScaler()\n",
    "    data[numeric_columns] = scaler.fit_transform(data[numeric_columns])\n",
    "    pca = PCA(n_components=n_components)\n",
    "    principal_components = pca.fit_transform(data)\n",
    "    return pd.DataFrame(data=principal_components, columns=[f'Principal_Component_{i+1}' for i in range(n_components)])\n",
    "\n",
    "# Function to handle dataset loading based on user input\n",
    "def load_data():\n",
    "    print(\"Choose the type of dataset you want to clean:\")\n",
    "    print(\"1. Tabular Data\")\n",
    "    print(\"2. Image Samples (Placeholder)\")\n",
    "    print(\"3. Audio Samples\")\n",
    "\n",
    "    choice = input(\"Enter the number corresponding to your dataset type: \")\n",
    "\n",
    "    if choice == \"1\":\n",
    "        return load_tabular_data()\n",
    "    elif choice == \"2\":\n",
    "        return load_image_data()\n",
    "    elif choice == \"3\":\n",
    "        return load_audio_data()\n",
    "    else:\n",
    "        print(\"Invalid choice. Please choose a valid dataset type (1, 2, or 3).\")\n",
    "        return None\n",
    "\n",
    "def cleaned_tabular():\n",
    "    initial_file = load_data()\n",
    "    if initial_file is not None:\n",
    "        print(\"\\nOriginal Dataset:\")\n",
    "        #initial_file.to_csv(\"original_dataset.csv\", index=False)\n",
    "        #print(initial_file)\n",
    "\n",
    "        tabular_data = reduce_missing_values(initial_file)\n",
    "        tabular_data.to_csv(\"dataset_after_missing_values.csv\", index=False)\n",
    "        #print(\"\\nDataset after reducing missing values:\")\n",
    "        #print(tabular_data.head(10))\n",
    "\n",
    "        tabular_data_drop_highly_corr = drop_highly_correlated_columns(tabular_data)\n",
    "        #print(\"\\nDataset after dropping highly correlated columns:\")\n",
    "        #print(tabular_data_drop_highly_corr.head(10))\n",
    "        \n",
    "        tabular_data_perform_pca = perform_pca(tabular_data_drop_highly_corr, n_components=2) \n",
    "        tabular_data_perform_pca.to_csv(\"final_cleaned_dataset.csv\", index=False)\n",
    "\n",
    "        #print(\"\\n Dataset after doing pca\")\n",
    "        #print(tabular_data_perform_pca)\n",
    "        # Save the cleaned data to a new file\n",
    "        tabular_data_perform_pca.to_csv(\"data_after_pca\", index=False)\n",
    "cleaned_tabular()\n",
    "\n",
    "# Function to load image data\n",
    "def load_image_data(folder_path):\n",
    "    data_path = input(\"Please enter the path to your tabular data file (CSV, Excel, etc.): \")\n",
    "    try:\n",
    "        # Get a list of all files in the folder\n",
    "        image_files = [f for f in os.listdir(folder_path) if f.lower().endswith(('.jpg', '.jpeg', '.png', '.gif'))]\n",
    "\n",
    "        # Iterate through each image file and display it\n",
    "        for image_file in image_files:\n",
    "            image_path = os.path.join(folder_path, image_file)\n",
    "            image = Image.open(image_path)\n",
    "            image.show()\n",
    "    except Exception as e:\n",
    "        print(f\"Error opening images: {e}\")\n",
    "def apply_sharpening_smoothing(folder_path):\n",
    "    try:\n",
    "        # Iterate over all files in the folder\n",
    "        for filename in os.listdir(folder_path):\n",
    "            if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.gif')):\n",
    "                # Construct the full path to the image\n",
    "                image_path = os.path.join(folder_path, filename)\n",
    "                \n",
    "                # Open the image\n",
    "                image = Image.open(image_path)\n",
    "                \n",
    "                # Apply sharpening\n",
    "                sharpened_image = image.filter(ImageFilter.SHARPEN)\n",
    "                \n",
    "                # Apply smoothing\n",
    "                smoothed_image = image.filter(ImageFilter.SMOOTH)\n",
    "                \n",
    "                # Display the sharpened and smoothed images (optional)\n",
    "                sharpened_image.show()\n",
    "                smoothed_image.show()\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error applying smoothing: {e}\")\n",
    "\n",
    "def apply_histogram_equalization(folder_path):\n",
    "    try:\n",
    "        # Iterate over all files in the folder\n",
    "        for filename in os.listdir(folder_path):\n",
    "            if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.gif')):\n",
    "                # Construct the full path to the image\n",
    "                image_path = os.path.join(folder_path, filename)\n",
    "                \n",
    "                # Open the image\n",
    "                image = Image.open(image_path)\n",
    "                \n",
    "                # Convert the image to grayscale if it's in color\n",
    "                if image.mode != 'L':\n",
    "                    image = ImageOps.grayscale(image)\n",
    "                \n",
    "                # Perform histogram equalization\n",
    "                equalized_image = ImageOps.equalize(image)\n",
    "                \n",
    "                # Display the equalized image (optional)\n",
    "                equalized_image.show()\n",
    "    except Exception as e:\n",
    "        print(f\"Error applying histogram equalization: {e}\")        \n",
    "\n",
    "\n",
    "def generate_missing_regions(folder_path, missing_region_size=100):\n",
    "    try:\n",
    "        # Iterate over all files in the folder\n",
    "        for filename in os.listdir(folder_path):\n",
    "            if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.gif')):\n",
    "                # Construct the full path to the image\n",
    "                image_path = os.path.join(folder_path, filename)\n",
    "                \n",
    "                # Open the image\n",
    "                image = Image.open(image_path)\n",
    "                \n",
    "                # Get image dimensions\n",
    "                width, height = image.size\n",
    "                \n",
    "                # Generate random coordinates for the missing region\n",
    "                x = random.randint(0, width - missing_region_size)\n",
    "                y = random.randint(0, height - missing_region_size)\n",
    "                \n",
    "                # Create a solid color (you can replace this with a pattern or another image)\n",
    "                missing_region_color = (255, 255, 255)  # White color\n",
    "                \n",
    "                # Paste the missing region onto the image\n",
    "                image.paste(missing_region_color, (x, y, x + missing_region_size, y + missing_region_size))\n",
    "                \n",
    "                # Save the image with the missing region\n",
    "                output_path = os.path.join(folder_path, f\"missing_{filename}\")\n",
    "                image.save(output_path)\n",
    "                \n",
    "                # Display the original and modified images (optional)\n",
    "                image.show()\n",
    "                Image.open(output_path).show()\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error generating missing regions: {e}\")     \n",
    "\n",
    "def apply_average_blur(image_folder):\n",
    "    try:\n",
    "        # Iterate over all files in the folder\n",
    "        for filename in os.listdir(image_folder):\n",
    "            if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.gif')):\n",
    "                # Construct the full path to the image\n",
    "                image_path = os.path.join(image_folder, filename)\n",
    "                \n",
    "                # Open the image\n",
    "                image = Image.open(image_path)\n",
    "                \n",
    "                # Apply Average blur\n",
    "                average_blur_image = image.filter(ImageFilter.BLUR)\n",
    "                save_path = os.path.join(image_folder, f\"average_blur_{filename}\")\n",
    "                average_blur_image.save(save_path)\n",
    "                \n",
    "                # Display the original and modified images (optional)\n",
    "                image.show()\n",
    "                average_blur_image.show()\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error applying Average blur: {e}\")\n",
    "        \n",
    "def cleaned_image():\n",
    "    folder_path = load_image_data()\n",
    "    #generate_missing_regions(folder_path, missing_region_size=100)\n",
    "    #apply_histogram_equalization()\n",
    "    apply_average_blur(folder_path)\n",
    "cleaned_image()\n",
    "\n",
    "\n",
    "# Function to load audio samples\n",
    "def load_audio_data():\n",
    "    audio_path = input(\"Please enter the path to your audio file: \")\n",
    "    try:\n",
    "        audio = AudioSegment.from_file(audio_path)\n",
    "        return audio.raw_data, audio.frame_rate\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading audio data: {e}\")\n",
    "        return None "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
